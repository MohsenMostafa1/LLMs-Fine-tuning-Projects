{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Fine Tuning With GPT-Neo\n",
        "\n",
        "Data Handling: The code fetches text data and labels, tokenizes them, and converts them into a dataset.\n",
        "Model Preparation: Loads a GPT-Neo model configured for sequence classification and adapts it for handling padded inputs.\n",
        "Training: Sets up training arguments and uses the Trainer class to fine-tune the model on the IMDb dataset.\n",
        "Saving: The fine-tuned model and tokenizer are saved for later deployment or further use."
      ],
      "metadata": {
        "id": "ce86MWZynQYh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers peft datasets accelerate\n",
        "!pip install transformers datasets peft\n",
        "\n",
        "from IPython.display import clear_output\n",
        "clear_output()"
      ],
      "metadata": {
        "id": "t6iuCTGCn3NU"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "AutoTokenizer: This is used to tokenize the text data. In this case, it's specifically for GPT-Neo.\n",
        "GPTNeoForSequenceClassification: This loads the GPT-Neo model configured for sequence classification tasks.\n",
        "Trainer & TrainingArguments: These are utilities provided by the Hugging Face library to help with model training.\n",
        "Dataset: A utility from the datasets library for handling and processing data.\n",
        "requests: A Python library used to make HTTP requests, like fetching data from a URL.\n",
        "The URL points to a dataset hosted on the Hugging Face Dataset server.\n",
        "requests.get(url) sends a GET request to the URL.\n",
        "response.json() parses the returned data from JSON format into a Python dictionary."
      ],
      "metadata": {
        "id": "0BM05SgRrVRC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPTNeoForSequenceClassification, Trainer, TrainingArguments\n",
        "import requests\n",
        "from datasets import Dataset\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Fetch data from the URL\n",
        "url = \"https://datasets-server.huggingface.co/rows?dataset=stanfordnlp%2Fimdb&config=plain_text&split=train&offset=0&length=100\"\n",
        "response = requests.get(url)\n",
        "data = response.json()"
      ],
      "metadata": {
        "id": "EQEVUsfTnReQ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "texts: A list comprehension that extracts the 'text' field from each row in the dataset.\n",
        "labels: Another list comprehension that maps 'pos' labels to 1 and 'neg' labels to 0. This is necessary because the model expects numerical labels."
      ],
      "metadata": {
        "id": "GizivDwAq0L1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract texts and labels from the fetched data\n",
        "texts = [row['row']['text'] for row in data['rows']]\n",
        "labels = [1 if row['row']['label'] == 'pos' else 0 for row in data['rows']]  # Assuming 'pos' and 'neg' labels"
      ],
      "metadata": {
        "id": "7SFuBn3aq1kU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "AutoTokenizer.from_pretrained: Loads the pre-trained tokenizer associated with the GPT-Neo model.\n",
        "Adding a Padding Token: Checks if the tokenizer has a padding token. If not, it adds one. Padding tokens are used to ensure all sequences in a batch have the same length."
      ],
      "metadata": {
        "id": "YNF_ilLiq-TD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the tokenizer for GPT-Neo\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neo-125M\")"
      ],
      "metadata": {
        "id": "TPoF0Fovq9x4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "tokenizer: Converts the text data into token IDs that the model can process.\n",
        "truncation=True: If the text exceeds max_length, it will be truncated.\n",
        "padding=True: Pads shorter sequences to ensure all inputs in a batch are the same length.\n",
        "max_length=512: Sets the maximum token length for each sequence.\n",
        "Dataset.from_dict: Converts the tokenized inputs and labels into a Hugging Face Dataset object, which is a convenient format for model training."
      ],
      "metadata": {
        "id": "7FDy2YBzrF3p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Add a padding token if it doesn't exist\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "\n",
        "# Tokenize the data\n",
        "encodings = tokenizer(texts, truncation=True, padding=True, max_length=512)\n",
        "\n",
        "# Convert the data to a Hugging Face Dataset\n",
        "dataset = Dataset.from_dict({\n",
        "    'input_ids': encodings['input_ids'],\n",
        "    'attention_mask': encodings['attention_mask'],\n",
        "    'labels': labels\n",
        "})"
      ],
      "metadata": {
        "id": "m4td-CzArFUX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "train_test_split: Splits the dataset into 80% training and 20% testing sets.\n",
        "train_dataset & eval_dataset: These are the resulting training and testing datasets."
      ],
      "metadata": {
        "id": "Vk3CsFdXrTA_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare the dataset by splitting it into training and testing sets\n",
        "train_test_split = dataset.train_test_split(test_size=0.2)\n",
        "train_dataset = train_test_split['train']\n",
        "eval_dataset = train_test_split['test']"
      ],
      "metadata": {
        "id": "61QkuKPoqvg5"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "from_pretrained: Loads a pre-trained GPT-Neo model configured for sequence classification.\n",
        "num_labels=2: Specifies that the classification task has two possible labels (positive and negative).\n",
        "pad_token_id: Explicitly sets the model's pad_token_id to match the one used by the tokenizer. This ensures that the model correctly handles padded sequences.\n",
        "resize_token_embeddings: If new tokens were added to the tokenizer (e.g., padding token), the modelâ€™s embedding layer needs to be resized to accommodate the additional tokens."
      ],
      "metadata": {
        "id": "jJefFtVbrSRo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load GPT-Neo model for sequence classification\n",
        "model = GPTNeoForSequenceClassification.from_pretrained(\"EleutherAI/gpt-neo-125M\", num_labels=2)\n",
        "\n",
        "# Set the padding token ID in the model's configuration\n",
        "model.config.pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "# If you added a new padding token, resize the model embeddings\n",
        "model.resize_token_embeddings(len(tokenizer))\n"
      ],
      "metadata": {
        "id": "ITZqkwVVrR0Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TrainingArguments: Specifies the parameters for training:\n",
        "\n",
        "  output_dir: Directory to save model outputs.\n",
        "  evaluation_strategy: Evaluate the model at the end of every epoch.\n",
        "  learning_rate: The learning rate for the optimizer.\n",
        "  per_device_train_batch_size and per_device_eval_batch_size: Batch sizes for training and evaluation, respectively.\n",
        "  num_train_epochs: Number of times to iterate over the entire dataset during training.\n",
        "  weight_decay: Regularization technique to avoid overfitting.\n",
        "  logging_dir: Directory to store training logs.\n",
        "  logging_steps: How often to log training information.\n",
        "  save_total_limit: Limit the number of saved model checkpoints."
      ],
      "metadata": {
        "id": "6Caj7vjqrc4u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=4,   # Adjust based on your GPU memory\n",
        "    per_device_eval_batch_size=4,    # Adjust based on your GPU memory\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=10,\n",
        "    save_total_limit=2,\n",
        ")"
      ],
      "metadata": {
        "id": "FEEgmsPGrcEA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Trainer: The Trainer class is a high-level API for training Hugging Face models. It handles the training loop, evaluation, and other tasks.\n",
        "trainer.train(): Initiates the fine-tuning process.\n",
        "\n",
        "save_pretrained: Saves the fine-tuned model and tokenizer to the specified directory for future use."
      ],
      "metadata": {
        "id": "M_EL_ulPrbF1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Trainer setup\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        ")\n",
        "\n",
        "# Fine-tune the model\n",
        "trainer.train()\n",
        "\n",
        "# Save the fine-tuned model and tokenizer\n",
        "model.save_pretrained(\"./fine_tuned_gpt_neo\")\n",
        "tokenizer.save_pretrained(\"./fine_tuned_gpt_neo\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "id": "miQialnVnc6K",
        "outputId": "f43b9fd8-4209-4de6-a2ad-846fa00888cb"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of GPTNeoForSequenceClassification were not initialized from the model checkpoint at EleutherAI/gpt-neo-125M and are newly initialized: ['score.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [60/60 25:55, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.003800</td>\n",
              "      <td>0.000439</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000114</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000103</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('./fine_tuned_gpt_neo/tokenizer_config.json',\n",
              " './fine_tuned_gpt_neo/special_tokens_map.json',\n",
              " './fine_tuned_gpt_neo/vocab.json',\n",
              " './fine_tuned_gpt_neo/merges.txt',\n",
              " './fine_tuned_gpt_neo/added_tokens.json',\n",
              " './fine_tuned_gpt_neo/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "8-lR7qfWrjZP"
      }
    }
  ]
}